{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Chapter 10: Modeling Relationships #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.1\n",
    "Figure 10.1 in Section 10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "81cc57e31256b28d151b409c2371b9bba80ed7f8",
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "image/png": {
       "height": 1155,
       "width": 1602
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code Segment 10.1\n",
    "#Show Fisher z-transform\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.dpi']= 300\n",
    "\n",
    "def fisherz(rho):\n",
    "    #Fisher z-transform\n",
    "    return(.5*np.log((1+rho)/(1-rho)))\n",
    "\n",
    "x=np.arange(-.99,.9999,.01)\n",
    "plt.plot(x,fisherz(x),label=\"transform\")\n",
    "\n",
    "fzeq1=(np.exp(2)-1)/(np.exp(2)+1)  #argument where z-transform equals one\n",
    "plt.plot(x,x/fzeq1,label=\"linear to z=1\")\n",
    "plt.title(\"Figure 10.1: Fisher z-transform\")\n",
    "plt.xlabel(\"Correlation\")\n",
    "plt.ylabel(\"Fisher Z\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.2\n",
    "Pearson/Spearman calculations in Section 10.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X data: [1, -0.1, 1.5, 1.7, -1, 5]\n",
      "X ranks: [4. 5. 3. 2. 6. 1.]\n",
      "Y data: [1, -0.1, 1.5, 1.7, -1, -2]\n",
      "Y ranks: [3. 4. 2. 1. 5. 6.]\n",
      "Pearson correlation: -0.2844419046262926\n",
      "Spearman correlation from Pearson ranks: 0.14285714285714285\n",
      "Spearman from scipy: 0.14285714285714288\n"
     ]
    }
   ],
   "source": [
    "#Code Segment 10.2\n",
    "import scipy.stats as spst\n",
    "#Form ranks from Pearson example\n",
    "x=[1,-.1,1.5,1.7,-1,5]\n",
    "y=[1,-.1,1.5,1.7,-1,-2]\n",
    "\n",
    "print(\"X data:\",x)\n",
    "xr=7-spst.rankdata(x)\n",
    "print(\"X ranks:\",xr)\n",
    "\n",
    "print(\"Y data:\",y)\n",
    "yr=7-spst.rankdata(y)\n",
    "print(\"Y ranks:\",yr)\n",
    "\n",
    "pearson,psig=spst.pearsonr(x,y)\n",
    "#Compute Spearman by doing Pearson on ranks\n",
    "spearman,ssig=spst.pearsonr(xr,yr)\n",
    "#Direct call to Spearmsn\n",
    "spear_scipy,sspsig=spst.spearmanr(x,y)\n",
    "\n",
    "print(\"Pearson correlation:\",pearson)\n",
    "print(\"Spearman correlation from Pearson ranks:\",spearman)\n",
    "print(\"Spearman from scipy:\",spear_scipy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.3\n",
    "Acquire regional stock market indices in Section 10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputting  Europe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputting  North_America\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputting  Japan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1696  weekly observations starting 1990-07-04 ending 2022-12-28\n"
     ]
    }
   ],
   "source": [
    "#Code Segment 10.3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Get regional stock market index data from Ken French's website.\n",
    "#Convert daily to Wednesday-Wednesday weekly.\n",
    "\n",
    "ff_head='http://mba.tuck.dartmouth.edu/pages/faculty/ken.french/ftp/'\n",
    "ff_foot=\"_3_Factors_Daily_CSV.zip\"\n",
    "ff_names=[\"Europe\",\"North_America\",\"Japan\"]\n",
    "\n",
    "for name_index in range(len(ff_names)):\n",
    "    print(\"Inputting \",ff_names[name_index])\n",
    "    ffurl=ff_head+ff_names[name_index]+ff_foot\n",
    "    #Skip the header rows\n",
    "    df_region = pd.read_csv(ffurl, skiprows=6)\n",
    "    #Standardize name of Date column and market return column\n",
    "    col0=df_region.columns[0]\n",
    "    df_region.rename(columns={col0:'Date'},inplace=True)\n",
    "    df_region.rename(columns={\"Mkt-RF\":ff_names[name_index]},inplace=True)\n",
    "    #Merge into aggregate\n",
    "    if name_index == 0:\n",
    "        df_returns=df_region[df_region.columns[0:2]]\n",
    "    else:\n",
    "        df_returns = df_returns.merge(df_region[df_region.columns[0:2]], \n",
    "                            left_on='Date', right_on='Date')\n",
    "\n",
    "#Convert to log-returns\n",
    "df_logs_day=np.log(1+df_returns[df_returns.columns[1:]]/100)\n",
    "\n",
    "#Convert dates to datetime format\n",
    "df_logs_day.insert(0,\"Date\",df_returns[\"Date\"],True)\n",
    "df_logs_day[\"Date\"] = pd.to_datetime(df_logs_day[\"Date\"], format='%Y%m%d')\n",
    "        \n",
    "#Convert log-returns to weekly (Wednesday-Wednesday)\n",
    "#to avoid asynchronous trading effects\n",
    "df_logs_day = df_logs_day.set_index(\"Date\")\n",
    "df_logs=df_logs_day.resample('W-Wed').sum()\n",
    "#(Will include some holidays like July 4 and December 25, so a little off)\n",
    "\n",
    "#Remove the partial year at the end\n",
    "lastyear=df_logs.index[-1].year\n",
    "df_logs.drop(df_logs.index[df_logs.index.year==lastyear],axis=0,inplace=True)\n",
    "\n",
    "periodicity=52   #For use in later code segments\n",
    "\n",
    "nobs=len(df_logs)\n",
    "print(nobs,\" weekly observations starting\",df_logs.index[0].strftime(\"%Y-%m-%d\"), \\\n",
    "      \"ending\",df_logs.index[-1].strftime(\"%Y-%m-%d\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.4\n",
    "Regional stock market index correlation matrix in Section 10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation matrix and standard deviations (10.10):\n",
      "                 Europe  North_America     Japan\n",
      "Europe         1.000000       0.761644  0.510496\n",
      "North_America  0.761644       1.000000  0.412393\n",
      "Japan          0.510496       0.412393  1.000000\n",
      "Annualized standard deviations:\n",
      " [0.1857710991004088, 0.1690650344540889, 0.20216641309990663]\n",
      "Correlation significance: 0.024298867052249755\n"
     ]
    }
   ],
   "source": [
    "#Code Segment 10.4\n",
    "#Get and show correlation matrix and\n",
    "#standard deviations\n",
    "corr_matrix=df_logs[df_logs.columns].corr()\n",
    "cov_matrix=df_logs[df_logs.columns].cov()\n",
    "std_devs=[]\n",
    "for i in range(len(ff_names)):\n",
    "    #Annualize weekly data\n",
    "    std_devs.append(np.sqrt(periodicity*cov_matrix.iloc[i,i]))\n",
    "    \n",
    "print(\"Correlation matrix and standard deviations (10.8):\")\n",
    "print(corr_matrix)\n",
    "print('Annualized standard deviations:\\n',std_devs)\n",
    "zsig=np.sqrt(1/(nobs-3))\n",
    "rsig=(np.exp(2*zsig)-1)/(np.exp(2*zsig)+1)\n",
    "print('Correlation significance:',rsig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.5\n",
    "Regional stock market index global minimum variance portfolio in Section 10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum variance portfolio:\n",
      "Europe           0.095915\n",
      "North_America    0.575622\n",
      "Japan            0.328463\n",
      "Minimum variance portfolio annualized std deviation: 0.15289597855105871\n",
      "Lowest component annualized std deviation 0.1690650344540889 ( North_America )\n"
     ]
    }
   ],
   "source": [
    "#Code Segment 10.5\n",
    "#Compute minimum variance portfolio of three regions (4.8)\n",
    "cov_matrix_inverse=pd.DataFrame(np.linalg.pinv(cov_matrix.values), \\\n",
    "            cov_matrix.columns,cov_matrix.index)\n",
    "u=pd.Series([1]*len(cov_matrix_inverse),index=cov_matrix_inverse.index)\n",
    "minvport=cov_matrix_inverse.dot(u)\n",
    "minvar=1/minvport.dot(u)  #This is second part of formula (4.8)\n",
    "minvport*=minvar    #This is first part of formula (4.8)\n",
    "print('Minimum variance portfolio:')\n",
    "print(minvport.to_string())\n",
    "\n",
    "#Annualized standard deviation\n",
    "annminstd=np.sqrt(minvar*periodicity)\n",
    "print('Minimum variance portfolio annualized std deviation:',annminstd)\n",
    "#Find minimum component standard deviation\n",
    "compminstd=min(std_devs)\n",
    "comp_index=std_devs.index(compminstd)\n",
    "print(\"Lowest component annualized std deviation\",compminstd,\"(\",df_logs.columns[comp_index],\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.6\n",
    "Figure 10.2 in Section 10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cholesky:\n",
      " [[1.         0.         0.        ]\n",
      " [0.76164372 0.64799602 0.        ]\n",
      " [0.51049638 0.03638322 0.85910984]]\n"
     ]
    },
    {
     "data": {
      "image/png": "6c362b8a5c83e812b85a46116f5650bdf8b9cc68",
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "image/png": {
       "height": 1258,
       "width": 1607
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Error =  0.14189319376693255\n"
     ]
    }
   ],
   "source": [
    "#Code Segment 10.6\n",
    "#Generate graph of either simulated or historical sample correlation\n",
    "#from df_logs\n",
    "\n",
    "def make_corr_plot(df_logs, rtrial, samplesize, title_str, simulate):\n",
    "#Generate a multivariate normal distribution using the data in df_logs\n",
    "#compute sample correlations of size samplesize and graph them\n",
    "#simulate: False, use historical data in df_logs\n",
    "#          True, use simulated data in rtrial\n",
    "    nobs=len(df_logs)\n",
    "    corr_matrix=df_logs[df_logs.columns].corr()\n",
    "\n",
    "    #Get sample correlations\n",
    "    if simulate:\n",
    "        samplecorrs=[np.corrcoef(rtrial[i-samplesize:i].transpose()) \\\n",
    "                    for i in range(samplesize,nobs+1)]\n",
    "    else:\n",
    "        samplecorrs=[df_logs.iloc[i-samplesize:i][df_logs.columns].corr().values \\\n",
    "                    for i in range(samplesize,nobs+1)]\n",
    "    sccol=['r','g','b']\n",
    "    stride=int((nobs-periodicity+1)/(4*periodicity))*periodicity\n",
    "    \n",
    "    dates=df_logs.index[samplesize-1:]\n",
    "    plot_corrs(dates,samplecorrs,corr_matrix,sccol,stride, \\\n",
    "        title_str+str(samplesize)+'-week sample correlations')\n",
    "#Done with make_corr_plot\n",
    "    \n",
    "def plot_corrs(dates,corrs,corr_matrix,sccol,stride,title_str):\n",
    "    #dates and corrs have same length\n",
    "    #dates in datetime format\n",
    "    #corrs is a list of correlation matrices\n",
    "    #corr_matrix has the target correlations\n",
    "    #names of securities are the column names of corr_matrix\n",
    "    #sccol is colors for lines\n",
    "    #stride is how many dates to skip between ticks on x-axis\n",
    "    #title_str is title string\n",
    "\n",
    "    nobs=len(corrs)\n",
    "    nsecs=len(corrs[0])\n",
    "\n",
    "    #plot correlations in corrs, nsec per time period\n",
    "    ncorrs=nsecs*(nsecs-1)/2\n",
    "    z=0\n",
    "    #Go through each pair\n",
    "    for j in range(nsecs-1):\n",
    "        for k in range(j+1,nsecs):\n",
    "            #form time series of sample correlations\n",
    "            #for this pair of securities\n",
    "            cs=[corrs[i][j,k] for i in range(nobs)]\n",
    "            plt.plot(range(nobs),cs, \\\n",
    "                     label=corr_matrix.columns[j]+'/'+ \\\n",
    "                     corr_matrix.columns[k], \\\n",
    "                     color=sccol[z])\n",
    "            #Show target correlation in same color\n",
    "            line=[corr_matrix.iloc[j,k]]*(nobs)\n",
    "            plt.plot(range(nobs),line,color=sccol[z])\n",
    "            z+=1\n",
    "\n",
    "    plt.legend()\n",
    "    tix=[x.strftime(\"%Y-%m-%d\") for x in dates[0:nobs+1:stride]]\n",
    "    plt.xticks(range(0,nobs+1,stride),tix,rotation=45)\n",
    "    plt.ylabel(\"Correlation\")\n",
    "    plt.title(title_str)\n",
    "    plt.grid()\n",
    "    plt.show();\n",
    "#Done with plot_corrs\n",
    "         \n",
    "#Generate a simulation    \n",
    "#Show the Cholesky decomposition of the matrix\n",
    "chol=np.linalg.cholesky(corr_matrix)\n",
    "print('Cholesky:\\n',chol)\n",
    "\n",
    "#Generate random draws\n",
    "nobs=len(df_logs)\n",
    "nsecs=len(df_logs.columns)\n",
    "strial=np.random.normal(0,1,size=[nobs,nsecs])\n",
    "rtrial=np.matmul(chol,strial.T).T\n",
    "\n",
    "samplesize=periodicity\n",
    "title_str=\"Figure 10.2: Simulated \"\n",
    "simulate=True\n",
    "import matplotlib.pyplot as plt\n",
    "make_corr_plot(df_logs, rtrial, samplesize, title_str, simulate)\n",
    "print(\"Standard Error = \",np.tanh(1/np.sqrt(periodicity-3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.7\n",
    "Figure 10.3 in Section 10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "860c094706b46a0ea8b9b1cd1a817d7c19134fdb",
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "image/png": {
       "height": 1258,
       "width": 1607
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard Error =  0.08066953425829818\n"
     ]
    }
   ],
   "source": [
    "#Code Segment 10.7\n",
    "#Get sample 3-year simulated correlations\n",
    "samplesize=156\n",
    "title_str=\"Figure 10.3: Simulated \"\n",
    "simulate=True\n",
    "make_corr_plot(df_logs, rtrial, samplesize, title_str, simulate)\n",
    "print(\"Standard Error = \",np.tanh(1/np.sqrt(156-3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.8\n",
    "Figure 10.4 in Section 10.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "73fcf6639fe0f1b740f14a27b08cb51081d6e271",
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "image/png": {
       "height": 1258,
       "width": 1607
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code Segment 10.8\n",
    "#Get sample 3-year historical correlations\n",
    "samplesize=156\n",
    "title_str=\"Figure 10.4: Historical \"\n",
    "simulate=False\n",
    "make_corr_plot(df_logs, rtrial, samplesize, title_str, simulate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.9\n",
    "Figure 10.5 in Section 10.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "cb5668b32ab047f785465eaba27688547ce25b95",
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "execution_count": 10,
     "metadata": {
      "image/png": {
       "height": 1154,
       "width": 1620
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code Segment 10.9\n",
    "#Get FHFA house price index and graph it\n",
    "df_fhfa=pd.read_excel( \\\n",
    "        \"https://www.fhfa.gov/DataTools/Downloads/Documents/HPI/HPI_PO_summary.xls\", \\\n",
    "        skiprows=1)\n",
    "\n",
    "#Plot seasonally adjusted index\n",
    "seasonal_index=\"Seasonally-Adjusted Purchase-Only Index \\n(1991Q1=100)\"\n",
    "plt.plot(df_fhfa[\"Year\"],df_fhfa[seasonal_index])\n",
    "str_title=\"Figure 10.5: Seasonally Adjusted US House Price Index\\n\"\n",
    "str_title+=str(df_fhfa.iloc[0][\"Year\"])+\"Q\"+str(df_fhfa.iloc[0][\"Quarter\"])+\"-\"\n",
    "str_title+=str(df_fhfa.iloc[-1][\"Year\"])+\"Q\"+str(df_fhfa.iloc[-1][\"Quarter\"])\n",
    "plt.ylabel(\"HPI\")\n",
    "plt.title(str_title)\n",
    "plt.grid()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.10\n",
    "Figure 10.6 in Section 10.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "6661ead4ad6202ea9036962a5530b6276cb6f700",
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "image/png": {
       "height": 1314,
       "width": 1642
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code Segment 10.10\n",
    "import pandas as pd\n",
    "import qrpm_funcs as qf\n",
    "import datetime as dt\n",
    "from datetime import datetime\n",
    "import scipy.stats as spst\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "firstday=\"1970-12-31\"\n",
    "lastday=qf.LastYearEnd()\n",
    "seriesnames=['WILL5000IND','DGS10']\n",
    "cdates,ratematrix=qf.GetFREDMatrix(seriesnames,\n",
    "                        startdate=firstday,enddate=lastday)\n",
    "\n",
    "#Put in dataframe - no na's, only monthends\n",
    "df_stockbond=pd.DataFrame(ratematrix,index=[datetime.strptime(cd,\"%Y-%m-%d\") \\\n",
    "                for cd in cdates]).dropna().resample('M').last()\n",
    "\n",
    "#Convert to log-returns\n",
    "#Assume Treasury rates given are for 10-year par bond\n",
    "#paying semiannually\n",
    "stock_logs=[]\n",
    "bond_logs=[]\n",
    "idx_stock=0\n",
    "idx_tsy=1\n",
    "T20=20.    #Number of semiannual periods in the 10-year bond\n",
    "oldrate=df_stockbond.iloc[0][idx_tsy]/200    #Semiannual rate\n",
    "for i in range(len(df_stockbond)-1):\n",
    "    stock_logs.append(np.log(df_stockbond.iloc[i+1][idx_stock]/ \\\n",
    "                             df_stockbond.iloc[i][idx_stock]))\n",
    "    #Use Formula 3.7 to get the duration of the par bond in semiannual periods\n",
    "    duration=qf.formula3p7(oldrate*100.,oldrate*100.,T20)\n",
    "    newrate=df_stockbond.iloc[i+1][idx_tsy]/200\n",
    "    #Return for the month: carry plus principal change\n",
    "    bond_return=oldrate/6-duration*(newrate-oldrate)\n",
    "    bond_logs.append(np.log(1+bond_return))\n",
    "    oldrate=newrate\n",
    "\n",
    "#bring dates back to strings    \n",
    "dates=[datetime.strftime(df_stockbond.index[i],\"%Y-%m-%d\") \\\n",
    "       for i in range(1,len(df_stockbond))]\n",
    "\n",
    "corrs=[]\n",
    "stride=36\n",
    "for i in range(len(stock_logs)-stride):\n",
    "    rho,p=spst.pearsonr(stock_logs[i:i+stride],bond_logs[i:i+stride])\n",
    "    corrs.append(rho)\n",
    "\n",
    "plt.plot(dates[stride:],corrs)\n",
    "nobs=len(corrs)\n",
    "stride_ticks=120\n",
    "plt.xticks(range(0,nobs+1,stride_ticks), \\\n",
    "           dates[stride:-1:stride_ticks], \\\n",
    "           rotation=45)\n",
    "\n",
    "#Shade insignificant area\n",
    "fsig=1/np.sqrt(stride-3)\n",
    "rsig=(np.exp(2*fsig)-1)/(np.exp(2*fsig)+1)\n",
    "\n",
    "low_sig=[-np.abs(rsig)]*nobs\n",
    "high_sig=[np.abs(rsig)]*nobs\n",
    "\n",
    "plt.fill_between(range(nobs), low_sig, high_sig, \\\n",
    "                 facecolor='gray', alpha=0.5, interpolate=True)\n",
    "\n",
    "str_title=\"Figure 10.6: US Stock/Bond \"+str(stride)+ \\\n",
    "          \"-month correlations\\n\"\n",
    "str_title+=dates[stride]+\" to \"+dates[-1]\n",
    "str_title+=\"   (Gray=insignificant)\"\n",
    "plt.ylabel(\"Correlation\")\n",
    "plt.title(str_title)\n",
    "plt.grid()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.11\n",
    "Figure 10.7 in Section 10.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "504 SPY tickers input,   1.195 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48618 SPY options input,  14.071 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "472 SPY tickers matched with options,   2.412 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7612 SPX options input,   2.277 seconds\n"
     ]
    },
    {
     "data": {
      "image/png": "11b5bee1795cb9eb4d49eb0ea8fe90afc7ab1d18",
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "image/png": {
       "height": 1211,
       "width": 1680
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPX implied: 0.15005000000000002\n",
      "Number greater: 472\n",
      "Average implied: 0.2865621115819209\n",
      "Implied correlation per formula (10.16): 0.3156811774122457\n"
     ]
    }
   ],
   "source": [
    "#Code Segment 10.11\n",
    "#Read current holdings of SPY Exchange-Traded-Fund (ETF)\n",
    "#This is very close to the actual S&P 500 index\n",
    "#There is a time mismatch - this is current while options information is from\n",
    "#yearend, so implied correlations will be off a little\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "#Function used below to get implied volatilities\n",
    "def find_implieds(df_ATM,quote_date,day_target):\n",
    "    #find options in dataframe df_ATM having maturities close to\n",
    "    #day_target days from quote_date. Return average implieds of those\n",
    "    #options and a string giving one of the expiration dates used.\n",
    "    this_implied=0\n",
    "    num_implied=0\n",
    "    #Keep increasing the number of days interval\n",
    "    #around day_target until some options are found\n",
    "    for interval in range(16,86,10):\n",
    "        for i_ATM in range(len(df_ATM)):\n",
    "            if abs( int( \\\n",
    "             (df_ATM.iloc[i_ATM].expiration-quote_date) \\\n",
    "             /np.timedelta64(1, 'D'))-day_target)<interval:\n",
    "                this_implied+=df_ATM.iloc[i_ATM].implied_volatility_1545\n",
    "                num_implied+=1\n",
    "                str_exp = str(df_ATM.iloc[i_ATM].expiration)[:10]\n",
    "        if num_implied != 0:\n",
    "            break\n",
    "\n",
    "    return(this_implied/num_implied,str_exp)\n",
    "#Done with find_implieds\n",
    "\n",
    "#\n",
    "# Step 1 - get SPY tickers\n",
    "#\n",
    "start_time = time.perf_counter()\n",
    "url=\"https://www.ssga.com/us/en/individual/etfs/library-content/\"+\\\n",
    "    \"products/fund-data/etfs/us/holdings-daily-us-en-spy.xlsx\"\n",
    "df_spy=pd.read_excel(url,skiprows=4,engine=\"openpyxl\")\n",
    "df_spy.dropna(thresh=3,inplace=True)   #Clean up junk at the end\n",
    "#Extract tickers and weights\n",
    "orig_spy_tickers=[df_spy[\"Ticker\"].iloc(0)[i].upper() for i in range(len(df_spy))]\n",
    "weights=[df_spy[\"Weight\"].iloc(0)[i] for i in range(len(df_spy))]\n",
    "weights/=sum(weights)    #force weights to add to 1\n",
    "end_time = time.perf_counter()\n",
    "print(\"%d SPY tickers input, %7.3f seconds\" % (len(weights),end_time-start_time))\n",
    "\n",
    "#\n",
    "# Step 2 - get options on the tickers in SPY\n",
    "# and compute ATM volatility about 75 days out\n",
    "#\n",
    "yearend_string=qf.LastYearEnd()[:4]\n",
    "df_spy_options=pd.read_excel(r\"SPY_UnderlyingOptionsEODCalcs_\"+ \\\n",
    "                             yearend_string+\"-12-31.xlsx\",engine=\"openpyxl\")\n",
    "end_time, start_time = time.perf_counter(), end_time\n",
    "print(\"%d SPY options input, %7.3f seconds\" % (len(df_spy_options), end_time-start_time))\n",
    "\n",
    "#Match up stock tickers with options\n",
    "spy_tickers=[]\n",
    "implieds=[]\n",
    "vis=[]   #vi's as in (10.16)\n",
    "for i in range(len(orig_spy_tickers)):\n",
    "    df_target=df_spy_options[df_spy_options.underlying_symbol == orig_spy_tickers[i]]\n",
    "    if len(df_target)>0:   #Did we find any options on this ticker?\n",
    "        spy_tickers.append(orig_spy_tickers[i])\n",
    "        this_price = df_target.active_underlying_price_1545.unique()[0]\n",
    "        quote_date = df_target.quote_date.unique()[0]\n",
    "        #Find closest to the money\n",
    "        min_to_money = min(abs(df_target.strike.unique()-this_price))\n",
    "        df_ATM=df_target[abs(df_target.strike-this_price)==min_to_money]\n",
    "        \n",
    "        this_avg, str_exp = find_implieds(df_ATM,quote_date,75)\n",
    "        implieds.append(this_avg)\n",
    "        vis.append(implieds[-1]*weights[i])\n",
    "\n",
    "end_time, start_time = time.perf_counter(), end_time\n",
    "print(\"%d SPY tickers matched with options, %7.3f seconds\" % (len(spy_tickers), \\\n",
    "                                                end_time-start_time))\n",
    "\n",
    "#\n",
    "# Step 3: Get SPX implied volatility about 75 days out\n",
    "#\n",
    "df_spx_options=pd.read_excel(r\"SPX_UnderlyingOptionsEODCalcs_\"+ \\\n",
    "                             yearend_string+\"-12-31.xlsx\", \\\n",
    "                             engine=\"openpyxl\")\n",
    "end_time, start_time = time.perf_counter(), end_time\n",
    "print(\"%d SPX options input, %7.3f seconds\" % (len(df_spx_options), \\\n",
    "                                               end_time-start_time))\n",
    "#Get SPX ATM volatility\n",
    "this_price = df_spx_options.active_underlying_price_1545.unique()[0]\n",
    "#Find closest to the money\n",
    "min_to_money = min(abs(df_spx_options.strike.unique()-this_price))\n",
    "df_ATM=df_spx_options[abs(df_spx_options.strike-this_price)==min_to_money]\n",
    "\n",
    "spx_ATM_60to90_implied, str_exp = find_implieds(df_ATM,quote_date,75)\n",
    "\n",
    "#Make histogram of implieds\n",
    "\n",
    "n, bins, patches = plt.hist(implieds,bins=50)\n",
    "plt.axvline(spx_ATM_60to90_implied, color='k', linestyle='dashed', linewidth=1)\n",
    "plt.annotate('S&P 500 implied vol', xy=(spx_ATM_60to90_implied, .9*max(n)), \n",
    "            xytext=(np.average(implieds)*1.5, .9*max(n)), va='center', \n",
    "            color='k', arrowprops=dict(color='k',width=1,headwidth=4))\n",
    "plt.annotate('Average implied vol', xy=(np.average(implieds), .7*max(n)), \n",
    "            xytext=(np.average(implieds)*1.5, .7*max(n)), va='center',\n",
    "            color='r', arrowprops=dict(color='r',width=1,headwidth=4))\n",
    "\n",
    "plt.axvline(np.average(implieds), color='r', linestyle='dashed', linewidth=1)\n",
    "plt.xlabel(\"Annualized Implied Vol\")\n",
    "plt.ylabel(\"Count (\"+str(len(implieds))+\" total)\")\n",
    "plt.title(\"Figure 10.7: Histogram of ATM implied vols of SPY components\\n Expiring \" \\\n",
    "          +str_exp+\", Quoted \"+str(quote_date)[:10])\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "#Show number greater than SPX implied\n",
    "n_greater = sum(iv > spx_ATM_60to90_implied for iv in implieds)\n",
    "print(\"SPX implied:\",spx_ATM_60to90_implied)\n",
    "print(\"Number greater:\",n_greater)\n",
    "print(\"Average implied:\",np.average(implieds))\n",
    "\n",
    "#Do calculation for formula 10.16\n",
    "var_port=spx_ATM_60to90_implied**2\n",
    "sum_vi2=sum([vi**2 for vi in vis])\n",
    "sum_vi_2=sum(vis)**2\n",
    "implied_corr=(var_port-sum_vi2)/(sum_vi_2-sum_vi2)\n",
    "print(\"Implied correlation per formula (10.16):\",implied_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.12\n",
    "Figure 10.8 in Section 10.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "2b381a1b318ac1242dcb4ee04f169fd9bc442541",
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "image/png": {
       "height": 1098,
       "width": 1594
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code Segment 10.12\n",
    "## Graph CBOE COR3N series\n",
    "## https://www.cboe.com/us/indices/dashboard/cor3m/\n",
    "## Captured February 26, 2022\n",
    "import pandas as pd\n",
    "\n",
    "df_cor3m=pd.read_excel(\"cboe_cor3m.xlsx\",engine=\"openpyxl\")\n",
    "plt.plot(range(len(df_cor3m)),df_cor3m[\"COR3M\"])\n",
    "plt.grid()\n",
    "plt.title('Figure 10.8: CBOE implied correlations COR3M')\n",
    "nobs=len(df_cor3m)\n",
    "stride=36\n",
    "plt.xticks(range(0,nobs,stride),df_cor3m[\"Date\"].iloc[0:nobs:stride])\n",
    "plt.xlim(0,nobs)\n",
    "plt.ylabel(\"Correlation (percent)\")\n",
    "#plt.ylim(25,90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.13\n",
    "Figure 10.9 in Section 10.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "c1f65734464ce3336eec825093704896f808f1b0",
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "execution_count": 14,
     "metadata": {
      "image/png": {
       "height": 1258,
       "width": 1885
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Europe a=0.1824 b=0.7684 c=0.00003543 AnnEquilibStd=0.1935\n",
      "North_America a=0.1699 b=0.7815 c=0.00002997 AnnEquilibStd=0.1791\n",
      "Japan a=0.1128 b=0.8367 c=0.00004145 AnnEquilibStd=0.2064\n"
     ]
    }
   ],
   "source": [
    "#Code Segment 10.13\n",
    "import scipy.stats as spst\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "#CHEAT! - get overall mean and standard deviation vectors\n",
    "#In practice, would need to do everything out of sample - \n",
    "#start with a learning sample, e.g.\n",
    "overallmean=np.mean(df_logs,axis=0)\n",
    "overallstd=np.std(df_logs)\n",
    "tickerlist=df_logs.columns\n",
    "\n",
    "#Get GARCH params for each ticker\n",
    "initparams=[.12,.85,.6]\n",
    "gparams=[qf.Garch11Fit(initparams,df_logs[ticker]) for ticker in tickerlist]\n",
    "\n",
    "minimal=10**(-20)\n",
    "stgs=[] #Save the running garch sigmas\n",
    "for it,ticker in enumerate(tickerlist):\n",
    "    a,b,c=gparams[it]\n",
    "    \n",
    "    #Create time series of sigmas\n",
    "    t=len(df_logs[ticker])\n",
    "    stdgarch=np.zeros(t)\n",
    "    stdgarch[0]=overallstd[ticker]\n",
    "    #Compute GARCH(1,1) stddev's from data given parameters\n",
    "    for i in range(1,t):\n",
    "        #Note offset - i-1 observation of data\n",
    "        #is used for i estimate of std deviation\n",
    "        previous=stdgarch[i-1]**2\n",
    "        var=c+b*previous+\\\n",
    "            a*(df_logs[ticker][i-1]-overallmean[ticker])**2\n",
    "        stdgarch[i]=np.sqrt(var)\n",
    "\n",
    "    #Save for later de-GARCHing\n",
    "    stgs.append(stdgarch)\n",
    "    stdgarch=100*np.sqrt(periodicity)*stgs[it]  #Annualize\n",
    "    plt.plot(range(len(stdgarch)),stdgarch,label=ticker)\n",
    "    \n",
    "plt.grid()\n",
    "plt.title('Figure 10.9: GARCH(1,1) annualized standard deviations '+ \\\n",
    "          min(df_logs.index.strftime(\"%Y%m\"))+'-'+ \\\n",
    "          str(max(df_logs.index.strftime(\"%Y%m\"))))\n",
    "plt.ylabel('GARCH SDs')\n",
    "plt.legend()\n",
    "stride=5*periodicity\n",
    "tix=[x.strftime(\"%Y-%m-%d\") for x in df_logs.index[0:len(df_logs)-1:stride]]\n",
    "plt.xticks(range(0,len(df_logs),stride),tix,rotation=45)\n",
    "plt.show()\n",
    "\n",
    "for it,ticker in enumerate(tickerlist):\n",
    "    print(ticker,'a=%1.4f' % gparams[it][0], \\\n",
    "               'b=%1.4f' % gparams[it][1], \\\n",
    "               'c=%1.8f' % gparams[it][2], \\\n",
    "               'AnnEquilibStd=%1.4f' % \\\n",
    "               np.sqrt(periodicity*gparams[it][2]/ \\\n",
    "                       (1-gparams[it][0]-gparams[it][1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.14\n",
    "deGARCHed statistics in Section 10.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Europe\n",
      "    DeGARCHed Mean: -0.006335465072757949\n",
      "    Raw annualized Std Dev: 0.185716323601916\n",
      "    DeGARCHed Std Dev: 1.000934838881549\n",
      "    Raw excess kurtosis: 5.72308609689704\n",
      "    DeGARCHed Excess Kurtosis: 1.5791736576196476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "North_America\n",
      "    DeGARCHed Mean: -0.014069088834356223\n",
      "    Raw annualized Std Dev: 0.16901518481878622\n",
      "    DeGARCHed Std Dev: 1.0002445570822687\n",
      "    Raw excess kurtosis: 5.580467633225819\n",
      "    DeGARCHed Excess Kurtosis: 2.500713051644463\n",
      "Japan\n",
      "    DeGARCHed Mean: -0.0112808838314883\n",
      "    Raw annualized Std Dev: 0.20210680336454045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    DeGARCHed Std Dev: 1.0011833061949984\n",
      "    Raw excess kurtosis: 2.1528985498298443\n",
      "    DeGARCHed Excess Kurtosis: 1.7804141414949157\n"
     ]
    }
   ],
   "source": [
    "#Code Segment 10.14\n",
    "#Display before and after deGARCHing statistics\n",
    "\n",
    "#Demeaned, DeGARCHed series go in dfeps\n",
    "dfeps=df_logs.sort_values(by=\"Date\").copy()\n",
    "for it,ticker in enumerate(tickerlist):\n",
    "    dfeps[ticker]-=overallmean[ticker]\n",
    "    for i in range(len(dfeps)):\n",
    "        dfeps[ticker].iloc[i]/=stgs[it][i]\n",
    "    print(ticker)\n",
    "    print('    DeGARCHed Mean:',np.mean(dfeps[ticker]))\n",
    "    \n",
    "    print('    Raw annualized Std Dev:',np.sqrt(periodicity)*overallstd[ticker])\n",
    "    print('    DeGARCHed Std Dev:',np.std(dfeps[ticker]))\n",
    "    \n",
    "    print('    Raw excess kurtosis:',spst.kurtosis(df_logs[ticker]))\n",
    "    print('    DeGARCHed Excess Kurtosis:',spst.kurtosis(dfeps[ticker]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.15\n",
    "Figure 10.10 in Section 10.8.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "1a596dfd0d8e365dc67c5c1fbc2a08453052d19f",
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "image/png": {
       "height": 1258,
       "width": 1753
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code Segment 10.15\n",
    "#Get sample 156-month de-GARCHED correlations\n",
    "samplesize=3*periodicity\n",
    "title_str=\"Figure 10.10: Historical de-GARCHed \"\n",
    "simulate=False\n",
    "make_corr_plot(dfeps, rtrial, samplesize, title_str, simulate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.16\n",
    "Figure 10.11 in Section 10.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal lambda: 0.011849775161762383\n",
      "Optimal objective function: 3346.489490908641\n",
      "Half-life (years): 1.1182169359574616\n"
     ]
    },
    {
     "data": {
      "image/png": "2765e03428525f138b8ae0a61f8f1f5860738b68",
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "image/png": {
       "height": 1258,
       "width": 1900
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code Segment 10.16\n",
    "#Compute integrated correlations\n",
    "\n",
    "InData=np.array(dfeps[tickerlist])\n",
    "\n",
    "def IntegratedCorrObj(s):\n",
    "    #Compute time series of quasi-correlation\n",
    "    #matrices from InData using integrated parameter\n",
    "    #xlam=exp(s)/(1+exp(s)); note this format removes\n",
    "    #the need to enforce bounds of xlam being between\n",
    "    #0 and 1. This is applied to formula 10.29.\n",
    "    #Standardize Q's and apply formula 10.34.\n",
    "    #Returns scalar 10.34\n",
    "    xlam=np.exp(s)\n",
    "    xlam/=1+xlam\n",
    "    obj10p34=0.\n",
    "    previousq=np.identity(len(InData[0]))\n",
    "    #Form new shock matrix\n",
    "    for i in range(len(InData)):\n",
    "        #standardize previous q matrix\n",
    "        #and compute contribution to objective\n",
    "        #function\n",
    "        stdmtrx=np.diag([1/np.sqrt(previousq[s,s]) for s in range(len(previousq))])\n",
    "        previousr=stdmtrx @ (previousq @ stdmtrx)\n",
    "        #objective function\n",
    "        obj10p34+=np.log(np.linalg.det(previousr))\n",
    "        shockvec=np.array(InData[i])\n",
    "        vec1=shockvec @ np.linalg.inv(previousr)\n",
    "        #This makes obj10p34 into a 1,1 matrix\n",
    "        obj10p34+=vec1 @ shockvec\n",
    "              \n",
    "        #Update q matrix\n",
    "        shockvec=np.mat(shockvec)\n",
    "        shockmat=np.matmul(shockvec.T,shockvec)\n",
    "        previousq=xlam*shockmat+(1-xlam)*previousq\n",
    "    return(obj10p34[0,0])\n",
    "#Done with IntegratedCorrObj\n",
    "\n",
    "result=minimize_scalar(IntegratedCorrObj)\n",
    "\n",
    "xlamopt=np.exp(result.x)\n",
    "xlamopt/=1+xlamopt\n",
    "print('Optimal lambda:',xlamopt)\n",
    "print('Optimal objective function:',result.fun)\n",
    "if xlamopt>=1 or xlamopt==0:\n",
    "    halflife=0\n",
    "else:\n",
    "    halflife=-np.log(2)/np.log(1-xlamopt)\n",
    "print('Half-life (years):',halflife/periodicity)\n",
    "\n",
    "#Compute integrated correlations\n",
    "nobs=len(InData)\n",
    "nsecs=len(InData[0])\n",
    "#Start quasi-correlation matrix series with identity\n",
    "previousq=np.identity(nsecs)\n",
    "rmatrices=[]\n",
    "for i in range(nobs):\n",
    "    stdmtrx=np.diag([1/np.sqrt(previousq[s,s]) for s in range(nsecs)])\n",
    "    rmatrices.append(np.matmul(stdmtrx,np.matmul(previousq,stdmtrx)))\n",
    "    shockvec=np.mat(np.array(InData[i]))\n",
    "    #Update q matrix\n",
    "    shockmat=np.matmul(shockvec.T,shockvec)\n",
    "    previousq=xlamopt*shockmat+(1-xlamopt)*previousq\n",
    "\n",
    "#Plot integrated correlations\n",
    "iccol=['r','g','b']\n",
    "xtitle='Figure 10.11: Integrated correlations λ=%1.5f' % xlamopt\n",
    "xtitle+=', '+min(df_logs.index.strftime(\"%Y-%m-%d\"))+':'+ \\\n",
    "                  max(df_logs.index.strftime(\"%Y-%m-%d\"))\n",
    "dates=df_logs.index\n",
    "stride=5*periodicity\n",
    "plot_corrs(dates,rmatrices,corr_matrix,iccol,stride,xtitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.17\n",
    "Figure 10.12 in Section 10.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "863d581285500cf55d495c50520846340e15bda1",
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "image/png": {
       "height": 1155,
       "width": 1687
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code Segment 10.17\n",
    "#Map of objective with respect to half-life\n",
    "\n",
    "halflife=int(halflife)\n",
    "delta_halflife=int(halflife/2)\n",
    "\n",
    "x=np.arange(halflife-delta_halflife,halflife+delta_halflife)\n",
    "y=[IntegratedCorrObj(np.log((.5)**(-1/h)-1)) for h in x]\n",
    "\n",
    "plt.plot(x/periodicity,y)\n",
    "plt.title(\"Figure 10.12: Objective Function as half-life changes\")\n",
    "plt.xlabel(\"Halflife in years\")\n",
    "plt.ylabel(\"Objective\")\n",
    "plt.grid()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.18\n",
    "Figure 10.13 in Section 10.8.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/scipy/optimize/_hessian_update_strategy.py:182: UserWarning: delta_grad == 0.0. Check if the approximated function is linear. If the function is linear better results can be obtained by defining the Hessian as zero instead of using quasi-Newton approximations.\n",
      "  warn('delta_grad == 0.0. Check if the approximated '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal alpha, beta: 0.015336358982559405 0.9760831978440717\n",
      "Optimal objective function: 3323.457027587737\n",
      "Half-life (years): 0.5506466848231422\n"
     ]
    },
    {
     "data": {
      "image/png": "53c5a67d5c412569cccdbb7ececb98a5fe2cb958",
      "text/plain": [
       "<Figure size 1800x1200 with 1 Axes>"
      ]
     },
     "execution_count": 19,
     "metadata": {
      "image/png": {
       "height": 1314,
       "width": 1750
      },
      "needs_background": "light"
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Code Segment 10.18\n",
    "def MeanRevCorrObj(params):\n",
    "    #Compute time series of quasi-correlation\n",
    "    #matrices from InData using mean reverting\n",
    "    #formula 10.30. Standardize them and apply\n",
    "    #formula 10.34. Returns scalar 10.34\n",
    "    \n",
    "    #Extract parameters\n",
    "    alpha,beta=params\n",
    "    #Enforce bounds\n",
    "    if alpha<0 or beta<0:\n",
    "        return(10**20)\n",
    "    elif (alpha+beta)>.999:\n",
    "        return(10**20)\n",
    "    obj10p34=0\n",
    "    #Initial omega is obtained through correlation targeting\n",
    "    Rlong=np.corrcoef(InData.T)\n",
    "    previousq=np.identity(len(InData[0]))\n",
    "    #Form new shock matrix\n",
    "    for i in range(len(InData)):\n",
    "        #standardize previous q matrix\n",
    "        #and compute contribution to objective\n",
    "        #function\n",
    "        stdmtrx=np.diag([1/np.sqrt(previousq[s,s]) \\\n",
    "                         for s in range(len(previousq))])\n",
    "        previousr=np.matmul(stdmtrx,np.matmul(previousq,stdmtrx))\n",
    "        #objective function\n",
    "        obj10p34+=np.log(np.linalg.det(previousr))\n",
    "        shockvec=np.array(InData[i])\n",
    "        vec1=np.matmul(shockvec,np.linalg.inv(previousr))\n",
    "        #This makes obj10p34 into a 1,1 matrix\n",
    "        obj10p34+=np.matmul(vec1,shockvec)\n",
    "              \n",
    "        #Update q matrix\n",
    "        shockvec=np.mat(shockvec)\n",
    "        shockmat=np.matmul(shockvec.T,shockvec)\n",
    "        previousq=(1-alpha-beta)*Rlong+alpha*shockmat+beta*previousq\n",
    "    return(obj10p34[0,0])\n",
    "#Done with MeanRevCorrObj\n",
    "\n",
    "import scipy.optimize as scpo\n",
    "#alpha and beta positive\n",
    "corr_bounds = scpo.Bounds([0,0],[np.inf,np.inf])\n",
    "#Sum of alpha and beta is less than 1\n",
    "corr_linear_constraint = \\\n",
    "    scpo.LinearConstraint([[1, 1]],[0],[.999])\n",
    "\n",
    "initparams=[.02,.93]\n",
    "\n",
    "results = scpo.minimize(MeanRevCorrObj, \\\n",
    "        initparams, \\\n",
    "        method='trust-constr', \\\n",
    "        jac='2-point', \\\n",
    "        hess=scpo.SR1(), \\\n",
    "        bounds=corr_bounds, \\\n",
    "        constraints=corr_linear_constraint)\n",
    "\n",
    "alpha,beta=results.x\n",
    "print('Optimal alpha, beta:',alpha,beta)\n",
    "print('Optimal objective function:',results.fun)\n",
    "halflife=-np.log(2)/np.log(beta)\n",
    "print('Half-life (years):',halflife/periodicity)\n",
    "\n",
    "#Compute mean reverting correlations\n",
    "nobs=len(InData)\n",
    "nsecs=len(InData[0])\n",
    "previousq=np.identity(nsecs)\n",
    "Rlong=np.corrcoef(InData.T)\n",
    "rmatrices=[]\n",
    "for i in range(nobs):\n",
    "    stdmtrx=np.diag([1/np.sqrt(previousq[s,s]) for s in range(nsecs)])\n",
    "    rmatrices.append(np.matmul(stdmtrx,np.matmul(previousq,stdmtrx)))\n",
    "    shockvec=np.mat(np.array(InData[i]))\n",
    "    #Update q matrix\n",
    "    shockmat=np.matmul(shockvec.T,shockvec)\n",
    "    previousq=(1-alpha-beta)*Rlong+alpha*shockmat+beta*previousq\n",
    "\n",
    "#Plot mean-reverting correlations\n",
    "iccol=['r','g','b']\n",
    "xtitle='Figure 10.13: Mean Reverting Correlations α=%1.5f' % alpha\n",
    "xtitle+=', β=%1.5f' % beta\n",
    "xtitle+=',\\n'+min(df_logs.index.strftime(\"%Y-%m-%d\"))+':'+ \\\n",
    "             max(df_logs.index.strftime(\"%Y-%m-%d\"))\n",
    "dates=df_logs.index\n",
    "stride=5*periodicity\n",
    "plot_corrs(dates,rmatrices,corr_matrix,iccol,stride,xtitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Code segment 10.19\n",
    "MacGyver method calculations in Section 10.8.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Europe North_America\n",
      "    Optimal lambda: 0.030413043920421285\n",
      "    Optimal objective function: 2156.7933064578374\n",
      "    Half-life (years): 0.4315914887152863\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Europe Japan\n",
      "    Optimal lambda: 0.010313609686724502\n",
      "    Optimal objective function: 2875.2528154492807\n",
      "    Half-life (years): 1.2857666875857823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "North_America Japan\n",
      "    Optimal lambda: 0.0068378704058871895\n",
      "    Optimal objective function: 3054.6117169028635\n",
      "    Half-life (years): 1.9427288036119008\n",
      "\n",
      "Median MacGyver lambda: 0.010313609686724502\n"
     ]
    }
   ],
   "source": [
    "#Code Segment 10.19\n",
    "#MacGyver method - pairwise integrated\n",
    "minimal=10**(-20)\n",
    "xlams=[]\n",
    "for it in range(len(tickerlist)-1):\n",
    "    tick1=tickerlist[it]\n",
    "    for jt in range(it+1,len(tickerlist)):\n",
    "        tick2=tickerlist[jt]\n",
    "        InData=np.array(dfeps[[tick1,tick2]])\n",
    "        result=minimize_scalar(IntegratedCorrObj)\n",
    "        xlamopt=np.exp(result.x)/(1+np.exp(result.x))\n",
    "        print(tick1,tick2)\n",
    "        print('    Optimal lambda:',xlamopt)\n",
    "        print('    Optimal objective function:', \\\n",
    "              result.fun)\n",
    "        if np.absolute(xlamopt)<minimal or xlamopt>=1:\n",
    "            halflife=0\n",
    "        else:\n",
    "            halflife=-np.log(2)/np.log(1-xlamopt)\n",
    "        print('    Half-life (years):',halflife/periodicity)\n",
    "        xlams.append(xlamopt)\n",
    "\n",
    "print('\\nMedian MacGyver lambda:',np.median(xlams))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {
   },
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}